{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18298de9",
   "metadata": {},
   "source": [
    "# Topic Modeling: Organizing Unlabeled CVs with LDA\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **Topic Modeling** using **Latent Dirichlet Allocation (LDA)** to organize unlabeled CVs (resumes) by automatically discovering hidden topics. Unlike supervised classification, topic modeling works with completely unlabeled data, making it ideal for organizing large document collections without manual labeling. You'll learn how to apply LDA to discover topics, interpret results, and organize documents based on their dominant topics.\n",
    "\n",
    "> \"The best way to find a needle in a haystack is to organize the haystack first.\"\n",
    "\n",
    "**The Problem**: You have a folder full of CVs—unlabeled, unorganized. You need to find candidates for specific roles, but manually reading through hundreds of CVs is impossible.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Understand what Topic Modeling is and why it's useful for unsupervised document organization\n",
    "- Learn how LDA (Latent Dirichlet Allocation) discovers hidden topics in text collections\n",
    "- Apply LDA to organize unlabeled documents automatically\n",
    "- Interpret topic modeling results by examining top words and document-topic distributions\n",
    "- Organize documents into folders based on their dominant topics\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. **Introduction to Topic Modeling** - What it is and why it's useful\n",
    "2. **What is LDA?** - Understanding Latent Dirichlet Allocation\n",
    "3. **The Pipeline** - Complete workflow from data loading to organization\n",
    "4. **Step 1: Loading Data** - Reading CVs from JSON files\n",
    "5. **Step 2: Preprocessing** - Cleaning and preparing text\n",
    "6. **Step 3: Vectorization** - Converting text to document-term matrix\n",
    "7. **Step 4: Training LDA** - Discovering topics automatically\n",
    "8. **Step 5: Analyzing Results** - Interpreting discovered topics\n",
    "9. **Step 6: Organizing Documents** - Creating folders and organizing CVs by topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded0fed",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "**Topic Modeling** is an **unsupervised learning** task that discovers hidden topics in a collection of unlabeled documents. Unlike classification (which requires labeled data), topic modeling finds patterns automatically.\n",
    "\n",
    "**Example applications:**\n",
    "- **Organizing unlabeled documents**: Group CVs by field (AI/ML, Data Analysis, etc.) without manual labeling\n",
    "- **Understanding large text collections**: Discover what themes exist in news archives, research papers, or social media\n",
    "- **Content recommendation**: Find documents similar to a given document based on topic similarity\n",
    "\n",
    "**Why it's useful:**\n",
    "- No labels needed: works with completely unlabeled data\n",
    "- Interpretable: topics are defined by their top words, making them understandable\n",
    "- Scalable: can process large document collections\n",
    "- Flexible: number of topics can be adjusted based on the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f61c3f",
   "metadata": {},
   "source": [
    "## What is LDA?\n",
    "\n",
    "**Latent Dirichlet Allocation (LDA)** is a probabilistic model that discovers hidden topics in a collection of documents.\n",
    "\n",
    "**Key idea**: \n",
    "- Each document is a **mixture of topics** (e.g., 70% AI/ML, 20% Data Analysis, 10% Software Engineering)\n",
    "- Each topic is a **distribution over words** (e.g., Topic 1: 30% \"PyTorch\", 25% \"TensorFlow\", 20% \"NLP\"...)\n",
    "- LDA discovers these topics automatically by finding words that co-occur together\n",
    "\n",
    "**For our CVs**: LDA will discover topics like \"AI/ML\", \"Data Analysis\", \"Big Data\" by looking at which words appear together, then assign each CV to the most relevant topic(s).\n",
    "\n",
    "**Reference**: Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). [Latent dirichlet allocation](https://dl.acm.org/doi/10.5555/944919.944937). *Journal of machine Learning research*, 3(Jan), 993-1022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf82c0",
   "metadata": {},
   "source": [
    "![Left: BoW. Right: LDA](../assets/lda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7018f6c",
   "metadata": {},
   "source": [
    "## The Pipeline\n",
    "\n",
    "1. **Load CVs**: Read all JSON files from topic folders using glob patterns and extract structured fields\n",
    "2. **Preprocess**: Clean the text (remove URLs, emails, etc.)\n",
    "3. **Vectorize**: Convert text to document-term matrix (Bag of Words)\n",
    "4. **Train LDA**: Discover topics automatically\n",
    "5. **Analyze Results**: See what topics were found and which CVs belong to each\n",
    "6. **Organize**: Create folders and copy CVs based on their dominant topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff53c5d",
   "metadata": {},
   "source": [
    "## Step 1: Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9332f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy==1.26.4 pandas==2.3.3 scikit-learn==1.8.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f5dc104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1aa196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 34 CV files from 3 topic folders:\n",
      "  1. 07\n",
      "  2. 10\n",
      "  3. 11\n",
      "  4. 15\n",
      "  5. 17\n",
      "  6. 20\n",
      "  7. 21\n",
      "  8. 22\n",
      "  9. 26\n",
      "  10. 29\n",
      "  11. 30\n",
      "  12. 33\n",
      "  13. 34\n",
      "  14. 39\n",
      "  15. 40\n",
      "  16. 08\n",
      "  17. 09\n",
      "  18. 12\n",
      "  19. 16\n",
      "  20. 18\n",
      "  21. 19\n",
      "  22. 23\n",
      "  23. 24\n",
      "  24. 31\n",
      "  25. 32\n",
      "  26. 35\n",
      "  27. 36\n",
      "  28. 37\n",
      "  29. 38\n",
      "  30. 13\n",
      "  31. 14\n",
      "  32. 25\n",
      "  33. 27\n",
      "  34. 28\n",
      "\n",
      "Combined structured data into text for 34 CVs\n"
     ]
    }
   ],
   "source": [
    "# Load CVs from JSON files in all topic folders\n",
    "cv_dir = Path('../datasets/CVs')\n",
    "# Use glob pattern to find all JSON files in Topic_* subdirectories, excluding English versions\n",
    "cv_files = sorted([f for f in cv_dir.glob('Topic_*/*.json') if not f.name.endswith('_en.json')])\n",
    "\n",
    "# Load and extract structured data from JSON\n",
    "cvs_data = []\n",
    "cv_names = []\n",
    "cv_file_paths = []  # Store original file paths for later copying\n",
    "\n",
    "for file in cv_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        cvs_data.append(data)\n",
    "        cv_names.append(file.stem)\n",
    "        cv_file_paths.append(file)  # Store the full path\n",
    "\n",
    "print(f\"Loaded {len(cvs_data)} CV files from {len(set(f.parent.name for f in cv_files))} topic folders:\")\n",
    "for i, name in enumerate(cv_names, 1):\n",
    "    print(f\"  {i}. {name}\")\n",
    "\n",
    "# Combine structured fields into text for each CV\n",
    "def combine_cv_fields(cv_json):\n",
    "    \"\"\"Combine Heading, Skills, Projects, Experience, Education into a single text\"\"\"\n",
    "    parts = []\n",
    "    \n",
    "    # Add heading\n",
    "    if 'Heading' in cv_json:\n",
    "        parts.append(cv_json['Heading'])\n",
    "    \n",
    "    # Add skills (join list items)\n",
    "    if 'Skills' in cv_json:\n",
    "        skills_text = ' '.join(cv_json['Skills']) if isinstance(cv_json['Skills'], list) else cv_json['Skills']\n",
    "        parts.append(skills_text)\n",
    "    \n",
    "    # Add projects\n",
    "    if 'Projects' in cv_json:\n",
    "        projects_text = ' '.join(cv_json['Projects']) if isinstance(cv_json['Projects'], list) else cv_json['Projects']\n",
    "        parts.append(projects_text)\n",
    "    \n",
    "    # Add experience\n",
    "    if 'Experience' in cv_json:\n",
    "        exp_text = ' '.join(cv_json['Experience']) if isinstance(cv_json['Experience'], list) else cv_json['Experience']\n",
    "        parts.append(exp_text)\n",
    "    \n",
    "    # Add education\n",
    "    if 'Education' in cv_json:\n",
    "        edu_text = ' '.join(cv_json['Education']) if isinstance(cv_json['Education'], list) else cv_json['Education']\n",
    "        parts.append(edu_text)\n",
    "    \n",
    "    return ' '.join(parts)\n",
    "\n",
    "# Convert JSON data to text\n",
    "cvs = [combine_cv_fields(cv_data) for cv_data in cvs_data]\n",
    "print(f\"\\nCombined structured data into text for {len(cvs)} CVs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbef1b5",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc5c7bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 34 CVs\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean text: remove URLs, emails, and normalize whitespace\"\"\"\n",
    "    # Remove emails and URLs\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Keep only Arabic/English letters and numbers\n",
    "    text = re.sub(r'[^\\w\\s\\u0600-\\u06FF]', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Preprocess all CVs\n",
    "cvs_processed = [preprocess_text(cv) for cv in cvs]\n",
    "print(f\"Preprocessed {len(cvs_processed)} CVs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e1f26b",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Data for LDA\n",
    "\n",
    "Convert text to a document-term matrix (same as Bag of Words from classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a63cfb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-Term Matrix: 34 CVs × 662 words\n",
      "Sparsity: 86.7%\n"
     ]
    }
   ],
   "source": [
    "# Create document-term matrix\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=1000,  # Top 1000 words\n",
    "    min_df=2,           # Word must appear in at least 2 CVs\n",
    "    max_df=0.8          # Ignore words in >80% of CVs\n",
    ")\n",
    "\n",
    "doc_term_matrix = vectorizer.fit_transform(cvs_processed)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Document-Term Matrix: {doc_term_matrix.shape[0]} CVs × {doc_term_matrix.shape[1]} words\")\n",
    "print(f\"Sparsity: {(1 - doc_term_matrix.nnz / (doc_term_matrix.shape[0] * doc_term_matrix.shape[1])) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29757a7c",
   "metadata": {},
   "source": [
    "## Step 4: Train LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "042a3137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA to discover 3 topics...\n",
      "✓ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train LDA model\n",
    "n_topics = 3  # Number of topics to discover\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    random_state=42,\n",
    "    max_iter=10,\n",
    "    learning_method='online'\n",
    ")\n",
    "\n",
    "print(f\"Training LDA to discover {n_topics} topics...\")\n",
    "lda.fit(doc_term_matrix)\n",
    "print(\"✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b719dc1",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Results\n",
    "\n",
    "Let's see what topics LDA discovered and which words define each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58b72bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 1:\n",
      "  Top words: engineer, analyst, analytics, 2019, 2021, business, with, 10, bi, governance\n",
      "  Weights: ['25.418', '25.095', '21.468', '20.625', '19.749', '19.336', '18.298', '16.163', '16.113', '15.502']\n",
      "\n",
      "Topic 2:\n",
      "  Top words: ai, on, models, engineer, research, computer, model, 06, 08, engineering\n",
      "  Weights: ['32.898', '28.287', '22.209', '20.499', '20.153', '20.043', '19.232', '16.574', '16.569', '15.889']\n",
      "\n",
      "Topic 3:\n",
      "  Top words: engineer, spark, big, hadoop, on, aws, platform, 2021, 01, time\n",
      "  Weights: ['7.766', '7.080', '5.884', '5.601', '4.821', '4.686', '4.546', '4.334', '4.237', '4.199']\n"
     ]
    }
   ],
   "source": [
    "# Display top words for each topic\n",
    "def display_topics(model, feature_names, n_top_words=10):\n",
    "    \"\"\"Display top words for each topic\"\"\"\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words_idx = topic.argsort()[-n_top_words:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        top_weights = [topic[i] for i in top_words_idx]\n",
    "        \n",
    "        print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "        print(\"  Top words:\", \", \".join(top_words))\n",
    "        print(\"  Weights:\", [f\"{w:.3f}\" for w in top_weights])\n",
    "\n",
    "display_topics(lda, feature_names, n_top_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66088b1",
   "metadata": {},
   "source": [
    "**Interpreting the topics**: Look at the top words for each topic. Can you guess what each topic represents? For example:\n",
    "- Topic with \"PyTorch\", \"TensorFlow\", \"NLP\" → probably AI/ML\n",
    "- Topic with \"Tableau\", \"Power BI\", \"dashboard\" → probably Data Analysis\n",
    "- Topic with \"Hadoop\", \"Spark\", \"Kafka\" → probably Big Data\n",
    "\n",
    "Now let's see which CV belongs to which topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f168fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Assignment to Topics:\n",
      "============================================================\n",
      "\n",
      "Topic 1 (17 CVs):\n",
      "  - 15 (99.4%)\n",
      "  - 29 (99.4%)\n",
      "  - 30 (99.3%)\n",
      "  - 08 (99.4%)\n",
      "  - 09 (99.5%)\n",
      "  - 18 (99.4%)\n",
      "  - 19 (99.4%)\n",
      "  - 23 (75.0%)\n",
      "  - 24 (79.7%)\n",
      "  - 31 (99.2%)\n",
      "  - 35 (99.3%)\n",
      "  - 36 (99.3%)\n",
      "  - 37 (99.3%)\n",
      "  - 38 (99.4%)\n",
      "  - 14 (99.5%)\n",
      "  - 27 (99.4%)\n",
      "  - 28 (99.4%)\n",
      "\n",
      "Topic 2 (14 CVs):\n",
      "  - 07 (99.5%)\n",
      "  - 10 (99.4%)\n",
      "  - 11 (99.5%)\n",
      "  - 17 (99.4%)\n",
      "  - 20 (99.4%)\n",
      "  - 21 (99.4%)\n",
      "  - 22 (99.4%)\n",
      "  - 26 (99.4%)\n",
      "  - 33 (99.4%)\n",
      "  - 34 (99.4%)\n",
      "  - 39 (99.4%)\n",
      "  - 40 (99.4%)\n",
      "  - 13 (84.1%)\n",
      "  - 25 (99.3%)\n",
      "\n",
      "Topic 3 (3 CVs):\n",
      "  - 12 (99.5%)\n",
      "  - 16 (99.3%)\n",
      "  - 32 (99.4%)\n"
     ]
    }
   ],
   "source": [
    "# Get topic distribution for each CV\n",
    "doc_topic_dist = lda.transform(doc_term_matrix)\n",
    "\n",
    "# Find dominant topic for each CV\n",
    "dominant_topics = doc_topic_dist.argmax(axis=1)\n",
    "\n",
    "# Create a DataFrame to see results\n",
    "df_results = pd.DataFrame({\n",
    "    'CV': cv_names,\n",
    "    'Dominant Topic': dominant_topics + 1,\n",
    "    'Topic Probabilities': [dist for dist in doc_topic_dist]\n",
    "})\n",
    "\n",
    "# Show which CVs belong to which topic\n",
    "print(\"CV Assignment to Topics:\")\n",
    "print(\"=\" * 60)\n",
    "for topic_id in range(n_topics):\n",
    "    topic_cvs = df_results[df_results['Dominant Topic'] == topic_id + 1]\n",
    "    print(f\"\\nTopic {topic_id + 1} ({len(topic_cvs)} CVs):\")\n",
    "    for idx, row in topic_cvs.iterrows():\n",
    "        prob = row['Topic Probabilities'][topic_id]\n",
    "        print(f\"  - {row['CV']} ({prob:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d086c24a",
   "metadata": {},
   "source": [
    "## Step 6: Organize CVs into Folders\n",
    "\n",
    "Now comes the practical part: **automatically organize CVs into folders** based on their dominant topic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8ee314d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 07.json → Topic_2/\n",
      "Copied 10.json → Topic_2/\n",
      "Copied 11.json → Topic_2/\n",
      "Copied 15.json → Topic_1/\n",
      "Copied 17.json → Topic_2/\n",
      "Copied 20.json → Topic_2/\n",
      "Copied 21.json → Topic_2/\n",
      "Copied 22.json → Topic_2/\n",
      "Copied 26.json → Topic_2/\n",
      "Copied 29.json → Topic_1/\n",
      "Copied 30.json → Topic_1/\n",
      "Copied 33.json → Topic_2/\n",
      "Copied 34.json → Topic_2/\n",
      "Copied 39.json → Topic_2/\n",
      "Copied 40.json → Topic_2/\n",
      "Copied 08.json → Topic_1/\n",
      "Copied 09.json → Topic_1/\n",
      "Copied 12.json → Topic_3/\n",
      "Copied 16.json → Topic_3/\n",
      "Copied 18.json → Topic_1/\n",
      "Copied 19.json → Topic_1/\n",
      "Copied 23.json → Topic_1/\n",
      "Copied 24.json → Topic_1/\n",
      "Copied 31.json → Topic_1/\n",
      "Copied 32.json → Topic_3/\n",
      "Copied 35.json → Topic_1/\n",
      "Copied 36.json → Topic_1/\n",
      "Copied 37.json → Topic_1/\n",
      "Copied 38.json → Topic_1/\n",
      "Copied 13.json → Topic_2/\n",
      "Copied 14.json → Topic_1/\n",
      "Copied 25.json → Topic_2/\n",
      "Copied 27.json → Topic_1/\n",
      "Copied 28.json → Topic_1/\n",
      "\n",
      "✓ Organization complete! CVs are now in: output/organized_cvs\n"
     ]
    }
   ],
   "source": [
    "# Create output directory structure\n",
    "output_dir = Path('output/organized_cvs')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create a folder for each topic\n",
    "for topic_id in range(n_topics):\n",
    "    topic_dir = output_dir / f\"Topic_{topic_id + 1}\"\n",
    "    topic_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Copy each CV to its topic folder\n",
    "for idx, (cv_name, topic_id, source_file) in enumerate(zip(cv_names, dominant_topics, cv_file_paths)):\n",
    "    target_dir = output_dir / f\"Topic_{topic_id + 1}\"\n",
    "    target_file = target_dir / f\"{cv_name}.json\"\n",
    "    \n",
    "    shutil.copy2(source_file, target_file)\n",
    "    print(f\"Copied {cv_name}.json → Topic_{topic_id + 1}/\")\n",
    "\n",
    "print(f\"\\n✓ Organization complete! CVs are now in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed5be03",
   "metadata": {},
   "source": [
    "### Verify the Organization\n",
    "\n",
    "Let's check what's in each folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "445a901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic_1/ (17 CVs):\n",
      "  - 08.json\n",
      "  - 09.json\n",
      "  - 14.json\n",
      "  - 15.json\n",
      "  - 18.json\n",
      "  - 19.json\n",
      "  - 23.json\n",
      "  - 24.json\n",
      "  - 27.json\n",
      "  - 28.json\n",
      "  - 29.json\n",
      "  - 30.json\n",
      "  - 31.json\n",
      "  - 35.json\n",
      "  - 36.json\n",
      "  - 37.json\n",
      "  - 38.json\n",
      "\n",
      "Topic_2/ (14 CVs):\n",
      "  - 07.json\n",
      "  - 10.json\n",
      "  - 11.json\n",
      "  - 13.json\n",
      "  - 17.json\n",
      "  - 20.json\n",
      "  - 21.json\n",
      "  - 22.json\n",
      "  - 25.json\n",
      "  - 26.json\n",
      "  - 33.json\n",
      "  - 34.json\n",
      "  - 39.json\n",
      "  - 40.json\n",
      "\n",
      "Topic_3/ (3 CVs):\n",
      "  - 12.json\n",
      "  - 16.json\n",
      "  - 32.json\n"
     ]
    }
   ],
   "source": [
    "# Show contents of each topic folder\n",
    "for topic_id in range(n_topics):\n",
    "    topic_dir = output_dir / f\"Topic_{topic_id + 1}\"\n",
    "    files = list(topic_dir.glob('*.json'))\n",
    "    print(f\"\\nTopic_{topic_id + 1}/ ({len(files)} CVs):\")\n",
    "    for f in sorted(files):\n",
    "        print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b48759d",
   "metadata": {},
   "source": [
    "## **Student Exercise**: discover topics on a dataset of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba6c05e",
   "metadata": {},
   "source": [
    "## Step 1: Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ffffa83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT EXERCISE\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"abisheksudarshan/topic-modeling-for-research-articles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46913c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Test.csv', 'Tags.csv', 'sample_sub.csv', 'Train.csv']\n",
      "(14004, 31)\n",
      "(6002, 6)\n",
      "(20006, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Analysis of PDEs</th>\n",
       "      <th>Applications</th>\n",
       "      <th>Artificial Intelligence</th>\n",
       "      <th>Astrophysics of Galaxies</th>\n",
       "      <th>...</th>\n",
       "      <th>Methodology</th>\n",
       "      <th>Number Theory</th>\n",
       "      <th>Optimization and Control</th>\n",
       "      <th>Representation Theory</th>\n",
       "      <th>Robotics</th>\n",
       "      <th>Social and Information Networks</th>\n",
       "      <th>Statistics Theory</th>\n",
       "      <th>Strongly Correlated Electrons</th>\n",
       "      <th>Superconductivity</th>\n",
       "      <th>Systems and Control</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1824</td>\n",
       "      <td>a ever-growing datasets inside observational astronomy have challenged scientists inside many aspects, including an efficient and interactive data exploration and visualization. many tools have been developed to confront this challenge. however, they usually focus on displaying a actual images or focus on visualizing patterns within catalogs inside the predefined way. inside this paper we introduce vizic, the python visualization library that builds a connection between images and catalogs through an interactive map of a sky region. vizic visualizes catalog data over the custom background canvas with the help of a shape, size and orientation of each object inside a catalog. a displayed objects inside a map are highly interactive and customizable comparing to those inside a images. these objects should be filtered by or colored by their properties, such as redshift and magnitude. they also should be sub-selected with the help of the lasso-like tool considering further analysis with the help of standard python functions from in the jupyter notebook. furthermore, vizic allows custom overlays to be appended dynamically on top of a sky map. we have initially implemented several overlays, namely, voronoi, delaunay, minimum spanning tree and healpix grid layers, which are helpful considering visualizing large-scale structure. all these overlays should be generated, added or removed interactively with one line of code. a catalog data was stored inside the non-relational database, and a interfaces were developed inside javascript and python to work within jupyter notebook, which allows to create custom widgets, user generated scripts to analyze and plot a data selected/displayed inside a interactive map. this unique design makes vizic the very powerful and flexible interactive analysis tool. vizic should be adopted inside variety of exercises, considering example, data inspection, clustering analysis, galaxy alignment studies, outlier identification or simply large-scale visualizations.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  \\\n",
       "0  1824   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ABSTRACT  \\\n",
       "0  a ever-growing datasets inside observational astronomy have challenged scientists inside many aspects, including an efficient and interactive data exploration and visualization. many tools have been developed to confront this challenge. however, they usually focus on displaying a actual images or focus on visualizing patterns within catalogs inside the predefined way. inside this paper we introduce vizic, the python visualization library that builds a connection between images and catalogs through an interactive map of a sky region. vizic visualizes catalog data over the custom background canvas with the help of a shape, size and orientation of each object inside a catalog. a displayed objects inside a map are highly interactive and customizable comparing to those inside a images. these objects should be filtered by or colored by their properties, such as redshift and magnitude. they also should be sub-selected with the help of the lasso-like tool considering further analysis with the help of standard python functions from in the jupyter notebook. furthermore, vizic allows custom overlays to be appended dynamically on top of a sky map. we have initially implemented several overlays, namely, voronoi, delaunay, minimum spanning tree and healpix grid layers, which are helpful considering visualizing large-scale structure. all these overlays should be generated, added or removed interactively with one line of code. a catalog data was stored inside the non-relational database, and a interfaces were developed inside javascript and python to work within jupyter notebook, which allows to create custom widgets, user generated scripts to analyze and plot a data selected/displayed inside a interactive map. this unique design makes vizic the very powerful and flexible interactive analysis tool. vizic should be adopted inside variety of exercises, considering example, data inspection, clustering analysis, galaxy alignment studies, outlier identification or simply large-scale visualizations.   \n",
       "\n",
       "   Computer Science  Mathematics  Physics  Statistics  Analysis of PDEs  \\\n",
       "0                 0            0        1           0               0.0   \n",
       "\n",
       "   Applications  Artificial Intelligence  Astrophysics of Galaxies  ...  \\\n",
       "0           0.0                      0.0                       0.0  ...   \n",
       "\n",
       "   Methodology  Number Theory  Optimization and Control  \\\n",
       "0          0.0            0.0                       0.0   \n",
       "\n",
       "   Representation Theory  Robotics  Social and Information Networks  \\\n",
       "0                    0.0       0.0                              0.0   \n",
       "\n",
       "   Statistics Theory  Strongly Correlated Electrons  Superconductivity  \\\n",
       "0                0.0                            0.0                0.0   \n",
       "\n",
       "   Systems and Control  \n",
       "0                  0.0  \n",
       "\n",
       "[1 rows x 31 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "print(os.listdir(path))\n",
    "# Load the dataset\n",
    "df_train = pd.read_csv(path + \"/Train.csv\")\n",
    "df_test = pd.read_csv(path + \"/Test.csv\")\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "# concatenate the two dataframes\n",
    "df = pd.concat([df_train, df_test])\n",
    "print(df.shape)\n",
    "# change the display option to show all the text\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# Display the first few rows of the dataset\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db565d5b",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b72940dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Analysis of PDEs</th>\n",
       "      <th>Applications</th>\n",
       "      <th>Artificial Intelligence</th>\n",
       "      <th>Astrophysics of Galaxies</th>\n",
       "      <th>...</th>\n",
       "      <th>Methodology</th>\n",
       "      <th>Number Theory</th>\n",
       "      <th>Optimization and Control</th>\n",
       "      <th>Representation Theory</th>\n",
       "      <th>Robotics</th>\n",
       "      <th>Social and Information Networks</th>\n",
       "      <th>Statistics Theory</th>\n",
       "      <th>Strongly Correlated Electrons</th>\n",
       "      <th>Superconductivity</th>\n",
       "      <th>Systems and Control</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1824</td>\n",
       "      <td>growing datasets inside observational astronomy challenged scientists inside aspects including efficient interactive data exploration visualization developed confront challenge usually focus displaying actual images focus visualizing patterns catalogs inside predefined way inside introduce vizic python visualization library builds connection images catalogs interactive map sky region vizic visualizes catalog data custom background canvas shape size orientation object inside catalog displayed objects inside map highly interactive customizable comparing inside images objects filtered colored properties redshift magnitude sub selected lasso like standard python functions jupyter notebook vizic custom overlays appended dynamically sky map initially implemented overlays voronoi delaunay minimum spanning tree healpix grid layers helpful visualizing large scale structure overlays generated added removed interactively line code catalog data stored inside non relational database interfaces developed inside javascript python jupyter notebook create custom widgets user generated scripts analyze plot data selected displayed inside interactive map unique design makes vizic powerful flexible interactive vizic adopted inside variety exercises example data inspection clustering galaxy alignment outlier identification simply large scale visualizations</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  \\\n",
       "0  1824   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ABSTRACT  \\\n",
       "0  growing datasets inside observational astronomy challenged scientists inside aspects including efficient interactive data exploration visualization developed confront challenge usually focus displaying actual images focus visualizing patterns catalogs inside predefined way inside introduce vizic python visualization library builds connection images catalogs interactive map sky region vizic visualizes catalog data custom background canvas shape size orientation object inside catalog displayed objects inside map highly interactive customizable comparing inside images objects filtered colored properties redshift magnitude sub selected lasso like standard python functions jupyter notebook vizic custom overlays appended dynamically sky map initially implemented overlays voronoi delaunay minimum spanning tree healpix grid layers helpful visualizing large scale structure overlays generated added removed interactively line code catalog data stored inside non relational database interfaces developed inside javascript python jupyter notebook create custom widgets user generated scripts analyze plot data selected displayed inside interactive map unique design makes vizic powerful flexible interactive vizic adopted inside variety exercises example data inspection clustering galaxy alignment outlier identification simply large scale visualizations   \n",
       "\n",
       "   Computer Science  Mathematics  Physics  Statistics  Analysis of PDEs  \\\n",
       "0                 0            0        1           0               0.0   \n",
       "\n",
       "   Applications  Artificial Intelligence  Astrophysics of Galaxies  ...  \\\n",
       "0           0.0                      0.0                       0.0  ...   \n",
       "\n",
       "   Methodology  Number Theory  Optimization and Control  \\\n",
       "0          0.0            0.0                       0.0   \n",
       "\n",
       "   Representation Theory  Robotics  Social and Information Networks  \\\n",
       "0                    0.0       0.0                              0.0   \n",
       "\n",
       "   Statistics Theory  Strongly Correlated Electrons  Superconductivity  \\\n",
       "0                0.0                            0.0                0.0   \n",
       "\n",
       "   Systems and Control  \n",
       "0                  0.0  \n",
       "\n",
       "[1 rows x 31 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean text: remove URLs, emails, and normalize whitespace\"\"\"\n",
    "    # Remove emails and URLs\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Keep only Arabic/English letters and numbers\n",
    "    text = re.sub(r'[^\\w\\s\\u0600-\\u06FF]', ' ', text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "# Import stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords from text including academic and generic terms\"\"\"\n",
    "    # Comprehensive academic stopwords list\n",
    "    academic_stopwords = [\n",
    "        # Generic academic connectors\n",
    "        'considering', 'using', 'therefore', 'furthermore', 'moreover', 'however', \n",
    "        'thus', 'hence', 'consequently', 'additionally', 'further', 'also',\n",
    "        \n",
    "        # Academic verbs (too generic)\n",
    "        'demonstrating', 'illustrating', 'showing', 'shows', 'show', 'present', \n",
    "        'presents', 'presented', 'propose', 'proposes', 'proposed', 'suggest', \n",
    "        'suggests', 'suggested', 'indicate', 'indicates', 'indicated',\n",
    "        \n",
    "        # Generic research terms\n",
    "        'model', 'models', 'approach', 'approaches', 'method', 'methods', \n",
    "        'methodology', 'study', 'studies', 'research', 'paper', 'papers',\n",
    "        'result', 'results', 'analysis', 'analyses', 'analysis', 'finding', 'findings',\n",
    "        \n",
    "        # Generic descriptors\n",
    "        'based', 'help', 'helps', 'helping', 'use', 'uses', 'used', 'using',\n",
    "        'provide', 'provides', 'provided', 'allow', 'allows', 'allowed',\n",
    "        'enable', 'enables', 'enabled', 'require', 'requires', 'required',\n",
    "        \n",
    "        # Generic academic phrases\n",
    "        'work', 'works', 'works', 'system', 'systems', 'application', 'applications',\n",
    "        'process', 'processes', 'technique', 'techniques', 'framework', 'frameworks',\n",
    "        'tool', 'tools', 'solution', 'solutions', 'problem', 'problems',\n",
    "        \n",
    "        # Common filler words in academic text\n",
    "        'one', 'two', 'three', 'first', 'second', 'third', 'new', 'novel',\n",
    "        'important', 'significant', 'effect', 'effects', 'different', 'various'\n",
    "    ]\n",
    "    \n",
    "    # Combine standard English stopwords with academic stopwords\n",
    "    stopwords = ENGLISH_STOP_WORDS.union(set(academic_stopwords))\n",
    "    \n",
    "    # Filter out stopwords and very short words (1-2 characters) which are usually noise\n",
    "    words = [word for word in text.split() \n",
    "             if word not in stopwords and len(word) > 2]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Clean abstract column\n",
    "df['ABSTRACT'] = df['ABSTRACT'].apply(preprocess_text)\n",
    "df['ABSTRACT'] = df['ABSTRACT'].apply(remove_stopwords)\n",
    "df.head(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e874a3b0",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Data for LDA\n",
    "\n",
    "Convert text to a document-term matrix (same as Bag of Words from classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "616460dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-Term Matrix: 20006 articles × 1000 words\n",
      "Sparsity: 96.6%\n"
     ]
    }
   ],
   "source": [
    "# Create document-term matrix\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=1000,  # Top 1000 words\n",
    "    min_df=2,           # Word must appear in at least 2 articles\n",
    "    max_df=0.7          # Ignore words in >80% of articles\n",
    ")\n",
    "\n",
    "doc_term_matrix = vectorizer.fit_transform(df['ABSTRACT'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Document-Term Matrix: {doc_term_matrix.shape[0]} articles × {doc_term_matrix.shape[1]} words\")\n",
    "print(f\"Sparsity: {(1 - doc_term_matrix.nnz / (doc_term_matrix.shape[0] * doc_term_matrix.shape[1])) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042eef1b",
   "metadata": {},
   "source": [
    "## Step 4: Train LDA Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a324aa0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA to discover 5 topics...\n",
      "✓ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train LDA model\n",
    "n_topics = 5  # Number of topics to discover\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    random_state=42,\n",
    "    max_iter=10,\n",
    "    learning_method='online'\n",
    ")\n",
    "\n",
    "print(f\"Training LDA to discover {n_topics} topics...\")\n",
    "lda.fit(doc_term_matrix)\n",
    "print(\"✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70525a7f",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Results\n",
    "\n",
    "Let's see what topics LDA discovered and which words define each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9869663a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 1:\n",
      "  Top words: phase, magnetic, spin, energy, field\n",
      "  Weights: ['2486.255', '2470.282', '2332.492', '2187.426', '1903.768']\n",
      "\n",
      "Topic 2:\n",
      "  Top words: learning, data, network, networks, neural\n",
      "  Weights: ['7582.480', '5749.925', '5724.820', '4419.499', '3660.568']\n",
      "\n",
      "Topic 3:\n",
      "  Top words: mass, galaxies, star, observations, data\n",
      "  Weights: ['2332.233', '1434.641', '1347.577', '1273.391', '1249.730']\n",
      "\n",
      "Topic 4:\n",
      "  Top words: space, flow, group, prove, mathbb\n",
      "  Weights: ['1452.993', '1390.706', '1389.660', '1352.290', '1339.774']\n",
      "\n",
      "Topic 5:\n",
      "  Top words: algorithm, data, time, approximation, algorithms\n",
      "  Weights: ['4734.481', '4564.084', '2868.735', '2631.065', '2469.988']\n"
     ]
    }
   ],
   "source": [
    "# Display top words for each topic\n",
    "def display_topics(model, feature_names, n_top_words=10):\n",
    "    \"\"\"Display top words for each topic\"\"\"\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words_idx = topic.argsort()[-n_top_words:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        top_weights = [topic[i] for i in top_words_idx]\n",
    "        \n",
    "        print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "        print(\"  Top words:\", \", \".join(top_words))\n",
    "        print(\"  Weights:\", [f\"{w:.3f}\" for w in top_weights])\n",
    "\n",
    "display_topics(lda, feature_names, n_top_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213d4f9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we accomplished**:\n",
    "1. ✅ Loaded unlabeled CVs from a folder\n",
    "2. ✅ Preprocessed the text data\n",
    "3. ✅ Created a document-term matrix\n",
    "4. ✅ Trained an LDA model to discover topics\n",
    "5. ✅ Analyzed which CVs belong to which topic\n",
    "6. ✅ **Automatically organized CVs into folders** based on discovered topics\n",
    "\n",
    "**Key Takeaways**:\n",
    "- **LDA discovers topics automatically** by finding words that co-occur together\n",
    "- **Each document is a mixture of topics** - LDA assigns probabilities\n",
    "- **Topic modeling is unsupervised** - no labels needed!\n",
    "- **Practical application**: Organize unlabeled documents automatically\n",
    "\n",
    "**Next Steps**:\n",
    "- Try different numbers of topics (`n_topics`) and see how results change\n",
    "- Experiment with preprocessing (stemming, stop words removal)\n",
    "- Use topic probabilities to handle CVs that belong to multiple topics\n",
    "- Visualize topics using tools like pyLDAvis\n",
    "\n",
    "**References**:\n",
    "- [Scikit-learn LDA documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)\n",
    "- [Topic modeling visualization guide](https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8790015",
   "metadata": {},
   "source": [
    "\n",
    "## Module 1 Synthesis: The Complete Pipeline\n",
    "\n",
    "Congratulations! You've completed **Module 1: Text Analysis with Statistical NLP**. Let's reflect on the journey and see how all the pieces fit together.\n",
    "\n",
    "### The Circular Learning Experience\n",
    "\n",
    "Remember the question chain we started with? Let's trace how we answered each question and built a complete NLP pipeline:\n",
    "\n",
    "1. **\"What is NLP?\"** → We learned that NLP bridges computers and human language, with applications in understanding and generation.\n",
    "\n",
    "2. **\"How do we extract patterns from text?\"** → We used **Regular Expressions** to find, match, and manipulate text patterns—essential for preprocessing.\n",
    "\n",
    "3. **\"How do we understand our data?\"** → We performed **Exploratory Data Analysis (EDA)** on corpora to assess data quality, vocabulary characteristics, and preprocessing needs.\n",
    "\n",
    "4. **\"How do we prepare text for ML?\"** → We applied **Preprocessing** techniques (cleaning, normalization, tokenization, stemming) to transform raw text into clean tokens.\n",
    "\n",
    "5. **\"How do we convert text to numbers?\"** → We used **Vectorization** (BoW, TF-IDF) to convert text into numerical features that ML models can process.\n",
    "\n",
    "6. **\"How do we build classifiers?\"** → We built **Text Classification** models (like sentiment analysis) using vectorized features and supervised learning.\n",
    "\n",
    "7. **\"How do we search documents?\"** → We implemented **Information Retrieval** systems using TF-IDF and cosine similarity to find relevant documents.\n",
    "\n",
    "8. **\"How do we discover topics?\"** → We applied **Topic Modeling** (LDA) to automatically organize unlabeled documents by discovering hidden topics.\n",
    "\n",
    "### The Complete NLP Pipeline\n",
    "\n",
    "Throughout this module, you've learned to build a complete NLP pipeline:\n",
    "\n",
    "```\n",
    "Raw Text\n",
    "    ↓\n",
    "[Regex: Pattern Extraction]\n",
    "    ↓\n",
    "[Corpus & EDA: Understanding Data]\n",
    "    ↓\n",
    "[Preprocessing: Cleaning & Normalization]\n",
    "    ↓\n",
    "[Vectorization: Text → Numbers]\n",
    "    ↓\n",
    "[Modeling: Classification / IR / Topic Modeling]\n",
    "    ↓\n",
    "Actionable Insights\n",
    "```\n",
    "\n",
    "### Key Skills You've Acquired\n",
    "\n",
    "By completing this module, you can now:\n",
    "\n",
    "✅ **Build supervised ML text classification pipelines**\n",
    "- Preprocess Arabic and English text\n",
    "- Vectorize text using BoW and TF-IDF\n",
    "- Train and evaluate classifiers\n",
    "- Interpret model results\n",
    "\n",
    "✅ **Apply keyword-based information retrieval**\n",
    "- Implement TF-IDF-based search engines\n",
    "- Measure document similarity using cosine similarity\n",
    "- Rank and retrieve relevant documents\n",
    "\n",
    "✅ **Apply unsupervised ML for document organization**\n",
    "- Discover hidden topics using LDA\n",
    "- Organize unlabeled documents automatically\n",
    "- Interpret topic modeling results\n",
    "\n",
    "### The Foundation for What's Next\n",
    "\n",
    "This module focused on **statistical NLP**—traditional methods that work well for many tasks. In **Module 2**, you'll learn about **Deep Learning approaches** (embeddings, transformers) that build on these foundations to achieve even better performance.\n",
    "\n",
    "**What you learned here is still valuable:**\n",
    "- Preprocessing techniques apply to both statistical and deep learning methods\n",
    "- Understanding vectorization helps you understand embeddings\n",
    "- EDA is always the first step, regardless of the approach\n",
    "- The pipeline structure (preprocess → vectorize → model) remains the same\n",
    "\n",
    "### Reflection Questions\n",
    "\n",
    "Before moving to Module 2, consider:\n",
    "\n",
    "1. **When would you use statistical NLP vs. deep learning?**\n",
    "   - Statistical NLP: Fast, interpretable, works with small data\n",
    "   - Deep Learning: Better accuracy, requires more data and computation\n",
    "\n",
    "2. **What preprocessing steps are most important?**\n",
    "   - Depends on your data and task, but EDA always guides the decision\n",
    "\n",
    "3. **How does TF-IDF differ from BoW?**\n",
    "   - BoW: Simple word counts\n",
    "   - TF-IDF: Weighted counts that emphasize distinctive words\n",
    "\n",
    "4. **When would you use topic modeling vs. classification?**\n",
    "   - Classification: When you have labels and want to predict categories\n",
    "   - Topic Modeling: When you have no labels and want to discover structure\n",
    "\n",
    "### The Journey Continues\n",
    "\n",
    "You've built a solid foundation in statistical NLP. The concepts you've learned—preprocessing, vectorization, classification, retrieval, and topic modeling—are the building blocks for more advanced techniques.\n",
    "\n",
    "**Next Module Preview:**\n",
    "- **Module 2** introduces **Deep Learning for NLP**:\n",
    "  - Tokenization with modern tools (WordPiece, BPE)\n",
    "  - Word embeddings (Word2Vec, GloVe, contextual embeddings)\n",
    "  - Transformers and BERT\n",
    "  - Fine-tuning pre-trained models\n",
    "\n",
    "The journey from statistical NLP to deep learning is a natural progression—you'll see how embeddings generalize vectorization, how transformers improve on traditional methods, and how pre-trained models leverage the foundations you've built.\n",
    "\n",
    "---\n",
    "\n",
    "**Module 1 Complete! 🎉**\n",
    "\n",
    "You now have the skills to work with text data using statistical methods. You understand the complete pipeline from raw text to actionable insights, and you're ready to explore the power of deep learning in Module 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
